# Amazon Fashion Reviews - Data Ingestion & Analysis Project

## ğŸ“‹ Project Overview

This project demonstrates a complete data ingestion and analysis pipeline for Amazon Fashion reviews using ClickHouse, Python, and modern data analytics tools. The system processes **2.5+ million reviews** with automated ingestion, comprehensive analysis, and detailed insights generation.

## ğŸ¯ Project Goals

- **Data Ingestion**: Automated ingestion of large-scale review data
- **Data Analysis**: Comprehensive exploratory data analysis (EDA)
- **Automation Design**: Conceptual framework for automated data processing
- **Performance**: High-throughput processing with duplicate prevention
- **Scalability**: Enterprise-ready architecture for big data scenarios

---

## ğŸ“ Project Structure

```
LONGIN-DUSENGEYEZU-ASSIGNEMNT/
â”œâ”€â”€ ğŸ“Š Data Files
â”‚   â”œâ”€â”€ Amazon_Fashion.csv          # Processed CSV data (2.5M+ records)
â”‚   â””â”€â”€ Amazon_Fashion.jsonl        # Original JSONL data
â”‚
â”œâ”€â”€ ğŸ Python Code
â”‚   â”œâ”€â”€ data_ingestion.py           # Main ingestion script
â”‚   â”œâ”€â”€ DataIngestion.ipynb         # Interactive ingestion notebook
â”‚   â””â”€â”€ AnalysisAndVisualization.ipynb # EDA and analysis notebook
â”‚
â”œâ”€â”€ ğŸ¤– Automation
â”‚   â””â”€â”€ AutomationChallenge.ipynb   # Automation design and architecture
â”‚
â”œâ”€â”€ ğŸ—„ï¸ Database & Infrastructure
â”‚   â”œâ”€â”€ docker-compose.yml          # ClickHouse container setup
â”‚   â”œâ”€â”€ .env                        # Environment configuration
â”‚   â””â”€â”€ clickhouse-config/          # ClickHouse configuration
â”‚
â”œâ”€â”€ ğŸ“Š Analysis & Documentation
â”‚   â”œâ”€â”€ sql_queries.sql             # All SQL queries used
â”‚   â”œâ”€â”€ insights_presentation.md    # Analysis insights summary
â”‚   â””â”€â”€ README.md                   # This file
â”‚
â””â”€â”€ ğŸ”§ Dependencies
    â”œâ”€â”€ python-dbutils/             # Custom database utilities
    â””â”€â”€ myenv/                      # Python virtual environment
```

---

## ğŸš€ Quick Start Guide

### Prerequisites

- **Docker** and **Docker Compose**
- **Python 3.10+**
- **Git**
- **8GB+ RAM** (recommended for large dataset processing)

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd LONGIN-DUSENGEYEZU-ASSIGNEMNT

# Start ClickHouse database
sudo docker-compose up -d

# Verify ClickHouse is running
sudo docker-compose ps
```

### 2. Python Environment

```bash
# Create virtual environment
python3 -m venv myenv
source myenv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install dbutils package
cd python-dbutils
pip install .
cd ..
```

### 3. Data Ingestion

```bash
# Activate virtual environment
source myenv/bin/activate

# Run data ingestion
python data_ingestion.py

# Or use Jupyter notebook
jupyter notebook DataIngestion.ipynb
```

### 4. Data Analysis

```bash
# Start Jupyter notebook
jupyter notebook

# Open AnalysisAndVisualization.ipynb
# Run all cells to generate insights and visualizations
```

---

## ğŸ“Š Dataset Information

### Amazon Fashion Reviews Dataset
- **Source**: UCSD McAuley Lab Amazon Reviews 2023
- **Records**: 2,531,859 reviews
- **Time Period**: 2002-2023 (22 years)
- **Products**: 874,297 unique items
- **Users**: 2,035,490 unique reviewers
- **Format**: JSONL â†’ CSV conversion

### Data Schema
```sql
CREATE TABLE amazon.reviews (
    rating Float32,           -- Product rating (1-5 stars)
    title String,            -- Review title
    text String,             -- Review content
    images String,           -- Image URLs (JSON array)
    asin String,             -- Amazon product identifier
    parent_asin String,      -- Parent product identifier
    user_id String,          -- User identifier
    timestamp UInt64,        -- Unix timestamp (milliseconds)
    helpful_vote UInt32,     -- Number of helpful votes
    verified_purchase UInt8, -- Purchase verification flag
    review_date Date MATERIALIZED toDate(timestamp / 1000),
    review_year UInt16 MATERIALIZED toYear(toDate(timestamp / 1000)),
    review_month UInt8 MATERIALIZED toMonth(toDate(timestamp / 1000))
) ENGINE = MergeTree()
ORDER BY (asin, timestamp, user_id)
```

---

## ğŸ”§ Technical Architecture

### Database Layer
- **ClickHouse**: High-performance OLAP database
- **Engine**: MergeTree for analytics optimization
- **Primary Key**: (asin, timestamp, user_id) for duplicate prevention
- **Materialized Columns**: Pre-computed date fields for performance

### Processing Layer
- **Python**: Primary development language
- **Pandas**: Data manipulation and analysis
- **Polars**: High-performance data processing
- **dbutils**: Custom database connectivity layer

### Infrastructure
- **Docker**: Containerized ClickHouse deployment
- **Virtual Environment**: Isolated Python dependencies
- **Configuration Management**: Environment-based settings

---

## ğŸ“ˆ Key Features

### Data Ingestion
- âœ… **Automated Processing**: Batch processing with configurable chunk sizes
- âœ… **Duplicate Prevention**: Primary key constraints and file-level checks
- âœ… **Error Handling**: Comprehensive error recovery and logging
- âœ… **Progress Monitoring**: Real-time progress tracking and performance metrics
- âœ… **Data Validation**: Schema validation and data quality checks

### Data Analysis
- âœ… **Exploratory Analysis**: Comprehensive EDA with statistical insights
- âœ… **Visualization**: Professional charts and graphs
- âœ… **Temporal Analysis**: Time-series trends and patterns
- âœ… **Performance Analytics**: Rating distributions and user behavior
- âœ… **Business Intelligence**: Actionable insights and recommendations

### Automation Design
- âœ… **File Detection**: Multiple monitoring strategies (watchdog, polling, events)
- âœ… **Scalability**: Auto-scaling architecture for high-volume processing
- âœ… **Big Data Integration**: Hadoop, Spark, Kafka ecosystem support
- âœ… **Cloud Deployment**: AWS, GCP, Azure integration patterns
- âœ… **Monitoring**: Comprehensive observability and alerting

---

## ğŸ“Š Analysis Results

### Customer Sentiment
- **Average Rating**: 4.2/5.0 stars
- **Positive Reviews**: 85.3% (4-5 stars)
- **Negative Reviews**: 6.4% (1-2 stars)
- **Overall Satisfaction**: High customer satisfaction levels

### Temporal Insights
- **Peak Activity**: 2018-2020 period
- **Seasonal Patterns**: Higher activity in Q4
- **Trend Stability**: Consistent rating patterns over time

### Product Performance
- **Quality Consistency**: High ratings across product categories
- **Review Volume**: Strong engagement with detailed feedback
- **User Behavior**: Active and authentic review community

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### ClickHouse Connection Issues
```bash
# Check if ClickHouse is running
sudo docker-compose ps

# Restart ClickHouse if needed
sudo docker-compose restart clickhouse

# Check logs
sudo docker-compose logs clickhouse
```

#### Python Environment Issues
```bash
# Recreate virtual environment
rm -rf myenv
python3 -m venv myenv
source myenv/bin/activate
pip install -r requirements.txt
```

#### Memory Issues
```bash
# Reduce batch size in data_ingestion.py
batch_size = 10000  # Instead of 50000

# Monitor system resources
htop
```

#### Database Permission Issues
```bash
# Check .env file configuration
cat .env

# Verify user permissions
# Default user: longin, password: longin@123
```

### Performance Optimization

#### For Large Datasets
- **Increase Memory**: Allocate 16GB+ RAM for processing
- **SSD Storage**: Use SSD for faster I/O operations
- **Batch Size**: Adjust batch_size parameter based on available memory
- **Parallel Processing**: Use multiple workers for ingestion

#### For Faster Queries
- **Materialized Views**: Pre-compute common aggregations
- **Indexing**: Optimize ClickHouse table ordering
- **Partitioning**: Partition large tables by date or category
- **Compression**: Enable compression for storage optimization

---

## ğŸ“š Dependencies

### Core Python Packages
```
pandas>=1.5.0
polars>=0.20.0
matplotlib>=3.5.0
seaborn>=0.11.0
numpy>=1.21.0
python-decouple>=3.6
jupyter>=1.0.0
```

### Database & Infrastructure
```
clickhouse-connect>=0.8.0
clickhouse-driver>=0.2.0
clickhouse-sqlalchemy>=0.3.0
docker>=6.0.0
```

### Big Data Technologies (Conceptual)
```
apache-spark>=3.3.0
apache-kafka>=3.2.0
apache-airflow>=2.5.0
apache-hadoop>=3.3.0
apache-hive>=3.1.0
```

---

## ğŸ”’ Security & Configuration

### Environment Variables
```bash
# .env file (never commit to version control)
CLICKHOUSE_DB=
CLICKHOUSE_USER=
CLICKHOUSE_PASSWORD=
CLICKHOUSE_HOST=
CLICKHOUSE_PORT=

# dbutils configuration
db_clickhouse_host=
db_clickhouse_port=
db_clickhouse_user=
db_clickhouse_pass=
db_clickhouse_db=
```

### Security Best Practices
- âœ… **No Hardcoded Credentials**: All secrets in .env file
- âœ… **Environment Isolation**: Virtual environment for dependencies
- âœ… **Container Security**: Docker containers with minimal privileges
- âœ… **Data Privacy**: Local processing without external data transmission

---

## ğŸ“Š Performance Benchmarks

### Ingestion Performance
- **Processing Speed**: ~50,000 rows/minute
- **Memory Usage**: ~2GB for 2.5M records
- **Storage**: ~500MB compressed data
- **Query Performance**: Sub-second response times

### System Requirements
- **Minimum RAM**: 8GB
- **Recommended RAM**: 16GB+
- **Storage**: 10GB+ free space
- **CPU**: 4+ cores recommended

---

## ğŸš€ Future Enhancements

### Planned Features
- **Real-time Streaming**: Kafka integration for live data
- **Machine Learning**: Predictive analytics and sentiment analysis
- **Advanced Visualizations**: Interactive dashboards
- **API Development**: RESTful API for data access
- **Cloud Deployment**: AWS/GCP/Azure integration

### Scalability Improvements
- **Distributed Processing**: Spark cluster integration
- **Auto-scaling**: Kubernetes-based scaling
- **Data Lake**: HDFS integration for petabyte-scale data
- **Streaming Analytics**: Real-time processing pipelines

---

## ğŸ“ Support & Contact

### Getting Help
1. **Check Documentation**: Review this README and code comments
2. **Examine Logs**: Check `data_ingestion.log` for detailed error information
3. **Verify Configuration**: Ensure .env file and Docker setup are correct
4. **Test Components**: Run individual components to isolate issues

### Assumptions
- **Docker Available**: Docker and Docker Compose installed and running
- **Python 3.10+**: Compatible Python version available
- **Sufficient Resources**: Adequate RAM and storage for large dataset processing
- **Network Access**: Internet connection for package installation

### Reproducibility
This project is designed for complete reproducibility:
- **Containerized Database**: Docker ensures consistent ClickHouse setup
- **Version Control**: All dependencies pinned to specific versions
- **Environment Isolation**: Virtual environment prevents conflicts
- **Configuration Management**: Environment-based settings for flexibility

---

## ğŸ“„ License & Attribution

### Data Source
- **Amazon Reviews Dataset**: UCSD McAuley Lab Amazon Reviews 2023
- **Original Research**: "Justifying recommendations using distantly-labeled reviews and fined-grained aspects" (McAuley et al.)

### Technologies Used
- **ClickHouse**: Yandex ClickHouse Team
- **Python**: Python Software Foundation
- **Docker**: Docker Inc.
- **Apache Projects**: Apache Software Foundation

---

*This project demonstrates enterprise-grade data ingestion and analysis capabilities using modern big data technologies. The complete pipeline processes millions of records with robust error handling, comprehensive analytics, and scalable architecture design.*
