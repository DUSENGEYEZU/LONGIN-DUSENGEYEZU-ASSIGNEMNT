{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fashion Reviews - Data Ingestion to ClickHouse\n",
    "\n",
    "This notebook demonstrates the complete data ingestion process for Amazon Fashion reviews into ClickHouse database using the dbutils package.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: Amazon Fashion Reviews (2.5M+ records)\n",
    "- **Source**: UCSD McAuley Lab Amazon Reviews 2023\n",
    "- **Target**: ClickHouse database with optimized schema\n",
    "- **Format**: CSV (converted from JSONL)\n",
    "- **Database Package**: dbutils (as per assignment requirements)\n",
    "\n",
    "## Tasks Covered\n",
    "1. âœ… ClickHouse schema creation (`amazon` database)\n",
    "2. âœ… Table creation with optimized data types and engine\n",
    "3. âœ… Data ingestion with batch processing using dbutils\n",
    "4. âœ… Duplicate prevention using primary keys\n",
    "5. âœ… Comprehensive logging and error handling\n",
    "6. âœ… Performance monitoring and verification\n",
    "\n",
    "## Prerequisites\n",
    "- ClickHouse running on localhost:9000 (native protocol)\n",
    "- Python packages: dbutils, pandas, python-decouple\n",
    "- Data file: Amazon_Fashion.csv\n",
    "- .env file with database credentials (no hardcoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up logging for our data ingestion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Logging configured for data ingestion process\n",
      " Using dbutils package for ClickHouse connectivity\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from decouple import AutoConfig\n",
    "from dbutils import Query\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_ingestion.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Logging configured for data ingestion process\")\n",
    "print(\" Using dbutils package for ClickHouse connectivity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ClickHouse Schema Setup\n",
    "\n",
    "Create the Amazon database and reviews table with optimized schema for analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ClickHouse connection...\n",
      "User: longin\n",
      "Host: localhost\n",
      "Port: 9000 (type: <class 'str'>)\n",
      " Connection successful: shape: (1, 1)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ test â”‚\n",
      "â”‚ ---  â”‚\n",
      "â”‚ i64  â”‚\n",
      "â•žâ•â•â•â•â•â•â•¡\n",
      "â”‚ 1    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Test ClickHouse Connection\n",
    "from decouple import AutoConfig\n",
    "from dbutils import Query\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/python-dbutils')\n",
    "\n",
    "config = AutoConfig(search_path='/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/.env')\n",
    "\n",
    "print('Testing ClickHouse connection...')\n",
    "print(f'User: {config(\"db_clickhouse_user\")}')\n",
    "print(f'Host: {config(\"db_clickhouse_host\")}')\n",
    "print(f'Port: {config(\"db_clickhouse_port\")} (type: {type(config(\"db_clickhouse_port\"))})')\n",
    "\n",
    "try:\n",
    "    clickhouse = Query(\n",
    "        db_type='clickhouse',\n",
    "        db=config('db_clickhouse_db'),\n",
    "        db_host=config('db_clickhouse_host'),\n",
    "        db_port=int(config('db_clickhouse_port')),  # Convert to int\n",
    "        db_user=config('db_clickhouse_user'),\n",
    "        db_pass=config('db_clickhouse_pass'),\n",
    "    )\n",
    "    \n",
    "    result = clickhouse.sql_query(sql='SELECT 1 as test')\n",
    "    print(f' Connection successful: {result}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f' Connection failed: {e}')\n",
    "    print('Please restart the kernel and try again')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:17:12,898 - INFO - ============================================================\n",
      "2025-09-13 21:17:12,899 - INFO - AMAZON FASHION REVIEWS DATA INGESTION STARTED\n",
      "2025-09-13 21:17:12,900 - INFO - ============================================================\n",
      "2025-09-13 21:17:12,901 - INFO -  ClickHouse connection established using dbutils\n",
      "2025-09-13 21:17:12,911 - INFO -  Amazon database created successfully\n",
      "2025-09-13 21:17:12,922 - INFO -  Reviews table created successfully with optimized schema\n",
      "2025-09-13 21:17:12,923 - INFO -  Table features:\n",
      "2025-09-13 21:17:12,924 - INFO -    - Engine: MergeTree (optimized for analytics)\n",
      "2025-09-13 21:17:12,924 - INFO -    - Ordering: (asin, timestamp, user_id) for duplicate prevention\n",
      "2025-09-13 21:17:12,925 - INFO -    - Materialized columns: review_date, review_year, review_month\n",
      "2025-09-13 21:17:12,926 - INFO -    - Primary key ensures no duplicate reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ClickHouse schema setup completed!\n",
      " Duplicate prevention: Primary key (asin, timestamp, user_id)\n"
     ]
    }
   ],
   "source": [
    "def setup_clickhouse_connection():\n",
    "    \"\"\"Setup ClickHouse connection using dbutils and .env configuration\"\"\"\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"AMAZON FASHION REVIEWS DATA INGESTION STARTED\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Load configurations from .env file (no hardcoding!)\n",
    "    config = AutoConfig(search_path=\"/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/.env\")\n",
    "    \n",
    "    # Initialize ClickHouse connection using dbutils\n",
    "    clickhouse = Query(\n",
    "        db_type=\"clickhouse\",\n",
    "        db=config(\"db_clickhouse_db\"),\n",
    "        db_host=config(\"db_clickhouse_host\"),\n",
    "        db_port=int(config(\"db_clickhouse_port\")),  # Convert port to integer\n",
    "        db_user=config(\"db_clickhouse_user\"),\n",
    "        db_pass=config(\"db_clickhouse_pass\"),\n",
    "    )\n",
    "    \n",
    "    logger.info(\" ClickHouse connection established using dbutils\")\n",
    "    \n",
    "    # Create Amazon database schema\n",
    "    try:\n",
    "        clickhouse.sql_query(sql=\"CREATE DATABASE IF NOT EXISTS amazon\")\n",
    "        logger.info(\" Amazon database created successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Error creating database: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Create reviews table with optimized schema for analytics\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS amazon.reviews (\n",
    "        rating Float32,\n",
    "        title String,\n",
    "        text String,\n",
    "        images String,\n",
    "        asin String,\n",
    "        parent_asin String,\n",
    "        user_id String,\n",
    "        timestamp UInt64,\n",
    "        helpful_vote UInt32,\n",
    "        verified_purchase UInt8,\n",
    "        review_date Date MATERIALIZED toDate(timestamp / 1000),\n",
    "        review_year UInt16 MATERIALIZED toYear(toDate(timestamp / 1000)),\n",
    "        review_month UInt8 MATERIALIZED toMonth(toDate(timestamp / 1000))\n",
    "    ) ENGINE = MergeTree()\n",
    "    ORDER BY (asin, timestamp, user_id)\n",
    "    SETTINGS index_granularity = 8192\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        clickhouse.sql_query(sql=create_table_sql)\n",
    "        logger.info(\" Reviews table created successfully with optimized schema\")\n",
    "        logger.info(\" Table features:\")\n",
    "        logger.info(\"   - Engine: MergeTree (optimized for analytics)\")\n",
    "        logger.info(\"   - Ordering: (asin, timestamp, user_id) for duplicate prevention\")\n",
    "        logger.info(\"   - Materialized columns: review_date, review_year, review_month\")\n",
    "        logger.info(\"   - Primary key ensures no duplicate reviews\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Error creating table: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return clickhouse\n",
    "\n",
    "# Execute schema setup\n",
    "clickhouse = setup_clickhouse_connection()\n",
    "print(\" ClickHouse schema setup completed!\")\n",
    "print(\" Duplicate prevention: Primary key (asin, timestamp, user_id)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion Function\n",
    "\n",
    "Implement batch processing for efficient data ingestion with comprehensive logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion functions defined\n",
      " Duplicate prevention: Primary key + data checking\n"
     ]
    }
   ],
   "source": [
    "def check_existing_data(clickhouse):\n",
    "    \"\"\"Check what data already exists to prevent duplicates\"\"\"\n",
    "    \n",
    "    logger.info(\"ðŸ” Checking existing data to prevent duplicates...\")\n",
    "    \n",
    "    try:\n",
    "        # Check total count\n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(*) as total_count FROM amazon.reviews\")\n",
    "        total_count = result[0][0] if result else 0\n",
    "        \n",
    "        # Check unique products and users\n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(DISTINCT asin) as unique_products FROM amazon.reviews\")\n",
    "        unique_products = result[0][0] if result else 0\n",
    "        \n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(DISTINCT user_id) as unique_users FROM amazon.reviews\")\n",
    "        unique_users = result[0][0] if result else 0\n",
    "        \n",
    "        # Check date range\n",
    "        result = clickhouse.sql_query(sql=\"SELECT MIN(review_date) as earliest, MAX(review_date) as latest FROM amazon.reviews\")\n",
    "        if result and result[0][0]:\n",
    "            earliest_date, latest_date = result[0]\n",
    "            logger.info(f\" Existing data: {total_count:,} reviews, {unique_products:,} products, {unique_users:,} users\")\n",
    "            logger.info(f\" Date range: {earliest_date} to {latest_date}\")\n",
    "        else:\n",
    "            logger.info(f\" Existing data: {total_count:,} reviews, {unique_products:,} products, {unique_users:,} users\")\n",
    "            logger.info(\" Date range: No data found\")\n",
    "        \n",
    "        return total_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\" Could not check existing data: {e}\")\n",
    "        return 0\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"Clean and preprocess DataFrame for ClickHouse insertion\"\"\"\n",
    "    \n",
    "    # Handle NaN values in string columns\n",
    "    string_columns = ['title', 'text', 'images', 'asin', 'parent_asin', 'user_id']\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "    \n",
    "    # Convert verified_purchase to UInt8 (0/1)\n",
    "    if 'verified_purchase' in df.columns:\n",
    "        df['verified_purchase'] = df['verified_purchase'].astype(int)\n",
    "    \n",
    "    # Ensure numeric columns are properly typed\n",
    "    numeric_columns = ['rating', 'timestamp', 'helpful_vote']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ingest_data_to_clickhouse(clickhouse, csv_file, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Ingest CSV data to ClickHouse with batch processing, duplicate prevention, and logging\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows_processed = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    logger.info(f\" Starting data ingestion...\")\n",
    "    logger.info(f\" Batch size: {batch_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV in chunks\n",
    "        chunk_iter = pd.read_csv(csv_file, chunksize=batch_size)\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            batch_count += 1\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Preprocess the chunk\n",
    "                chunk = preprocess_dataframe(chunk)\n",
    "                \n",
    "                # Write DataFrame to ClickHouse table using dbutils\n",
    "                clickhouse.sql_write(\n",
    "                    chunk,\n",
    "                    schema=\"amazon\",\n",
    "                    table_name=\"reviews\",\n",
    "                    max_chunk=batch_size,\n",
    "                    max_workers=1  # Single worker for consistency\n",
    "                )\n",
    "                \n",
    "                total_rows_processed += len(chunk)\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                \n",
    "                logger.info(f\" Batch {batch_count}: {len(chunk):,} rows processed in {batch_time:.2f}s\")\n",
    "                logger.info(f\" Total progress: {total_rows_processed:,} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\" Error processing batch {batch_count}: {e}\")\n",
    "                logger.error(f\" Failed batch size: {len(chunk):,} rows\")\n",
    "                raise\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\" Data ingestion completed successfully!\")\n",
    "        logger.info(f\" Total rows processed: {total_rows_processed:,}\")\n",
    "        logger.info(f\" Total time: {total_time:.2f} seconds\")\n",
    "        logger.info(f\" Average speed: {total_rows_processed/total_time:.0f} rows/second\")\n",
    "        \n",
    "        return total_rows_processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\" Critical error during ingestion: {e}\")\n",
    "        logger.error(f\" Rows processed before failure: {total_rows_processed:,}\")\n",
    "        raise\n",
    "\n",
    "print(\"Data ingestion functions defined\")\n",
    "print(\" Duplicate prevention: Primary key + data checking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Verification Function\n",
    "\n",
    "Verify the data ingestion with sample queries to ensure data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data verification function defined\n"
     ]
    }
   ],
   "source": [
    "def verify_data_ingestion(clickhouse):\n",
    "    \"\"\"Verify data ingestion with sample queries\"\"\"\n",
    "    \n",
    "    verification_queries = [\n",
    "        \"SELECT COUNT(*) as total_reviews FROM amazon.reviews\",\n",
    "        \"SELECT COUNT(DISTINCT asin) as unique_products FROM amazon.reviews\",\n",
    "        \"SELECT COUNT(DISTINCT user_id) as unique_users FROM amazon.reviews\",\n",
    "        \"SELECT AVG(rating) as avg_rating FROM amazon.reviews\",\n",
    "        \"SELECT MIN(review_date) as earliest_review, MAX(review_date) as latest_review FROM amazon.reviews\",\n",
    "        \"SELECT review_year, COUNT(*) as reviews_count FROM amazon.reviews GROUP BY review_year ORDER BY review_year\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\" VERIFYING DATA INGESTION...\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    for i, query in enumerate(verification_queries, 1):\n",
    "        try:\n",
    "            result = clickhouse.sql_query(sql=query)\n",
    "            logger.info(f\" Query {i}: {result}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Query {i} failed: {e}\")\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\" DATA VERIFICATION COMPLETED\")\n",
    "\n",
    "print(\" Data verification function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Data Ingestion\n",
    "\n",
    "Run the complete data ingestion process with the Amazon Fashion reviews dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data file found: Amazon_Fashion.csv (663.9 MB)\n",
      " Data structure verified - 10 columns\n",
      " Columns: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
      "\n",
      " Ready to start data ingestion!\n"
     ]
    }
   ],
   "source": [
    "# Check if data file exists\n",
    "csv_file = \"Amazon_Fashion.csv\"\n",
    "if not Path(csv_file).exists():\n",
    "    print(f\" Data file {csv_file} not found!\")\n",
    "    print(\"Please ensure the CSV file is in the current directory\")\n",
    "else:\n",
    "    # Get file size\n",
    "    file_size = Path(csv_file).stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\" Data file found: {csv_file} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    # Read first few rows to verify structure\n",
    "    sample_df = pd.read_csv(csv_file, nrows=5)\n",
    "    print(f\" Data structure verified - {len(sample_df.columns)} columns\")\n",
    "    print(f\" Columns: {list(sample_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n Ready to start data ingestion!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:17:12,976 - INFO - ðŸ” Checking existing data to prevent duplicates...\n",
      "2025-09-13 21:17:12,989 - WARNING -  Could not check existing data: the truth value of a DataFrame is ambiguous\n",
      "\n",
      "Hint: to check if a DataFrame contains any values, use `is_empty()`.\n",
      "2025-09-13 21:17:12,990 - INFO -  Starting data ingestion...\n",
      "2025-09-13 21:17:12,991 - INFO -  Batch size: 50,000 rows\n",
      "2025-09-13 21:17:13,300 - ERROR -  Error processing batch 1: 'DataFrame' object has no attribute 'to_pandas'\n",
      "2025-09-13 21:17:13,301 - ERROR -  Failed batch size: 50,000 rows\n",
      "2025-09-13 21:17:13,302 - ERROR -  Critical error during ingestion: 'DataFrame' object has no attribute 'to_pandas'\n",
      "2025-09-13 21:17:13,302 - ERROR -  Rows processed before failure: 0\n",
      "2025-09-13 21:17:13,303 - ERROR -  INGESTION FAILED: 'DataFrame' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ingestion failed: 'DataFrame' object has no attribute 'to_pandas'\n"
     ]
    }
   ],
   "source": [
    "# Execute the complete data ingestion process\n",
    "try:\n",
    "    # Check existing data to prevent duplicates\n",
    "    existing_count = check_existing_data(clickhouse)\n",
    "    \n",
    "    if existing_count > 0:\n",
    "        logger.info(f\" Found {existing_count:,} existing records\")\n",
    "        logger.info(\" Proceeding with ingestion (duplicates will be prevented by primary key)\")\n",
    "    \n",
    "    # Execute data ingestion\n",
    "    total_rows = ingest_data_to_clickhouse(clickhouse, csv_file, batch_size=50000)\n",
    "    \n",
    "    # Verify data ingestion\n",
    "    verify_data_ingestion(clickhouse)\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" AMAZON FASHION REVIEWS DATA INGESTION COMPLETED\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\" Database: amazon\")\n",
    "    logger.info(f\" Table: reviews\")\n",
    "    logger.info(f\" New records processed: {total_rows:,}\")\n",
    "    logger.info(f\" File processed: {csv_file}\")\n",
    "    logger.info(f\" Log file: data_ingestion.log\")\n",
    "    logger.info(\" Duplicate prevention: Enabled via primary key (asin, timestamp, user_id)\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n Data ingestion completed successfully!\")\n",
    "    print(f\" {total_rows:,} Amazon Fashion reviews ingested into ClickHouse\")\n",
    "    print(\" Duplicate prevention: Primary key ensures no duplicate reviews\")\n",
    "    print(\" Check 'data_ingestion.log' for detailed logs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\" INGESTION FAILED: {e}\")\n",
    "    print(f\" Ingestion failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Schema Design Details\n",
    "\n",
    "### Table Schema Explanation\n",
    "\n",
    "The `amazon.reviews` table is designed for optimal analytics performance and duplicate prevention:\n",
    "\n",
    "**Data Types:**\n",
    "- `rating`: Float32 - Product rating (1-5 stars)\n",
    "- `title`: String - Review title\n",
    "- `text`: String - Review content\n",
    "- `images`: String - JSON array of image URLs\n",
    "- `asin`: String - Amazon Standard Identification Number\n",
    "- `parent_asin`: String - Parent product ASIN\n",
    "- `user_id`: String - Unique user identifier\n",
    "- `timestamp`: UInt64 - Unix timestamp in milliseconds\n",
    "- `helpful_vote`: UInt32 - Number of helpful votes\n",
    "- `verified_purchase`: UInt8 - Boolean flag (0/1)\n",
    "\n",
    "**Materialized Columns:**\n",
    "- `review_date`: Date - Extracted from timestamp\n",
    "- `review_year`: UInt16 - Year for partitioning\n",
    "- `review_month`: UInt8 - Month for analytics\n",
    "\n",
    "**Engine Configuration:**\n",
    "- **Engine**: MergeTree (optimized for analytics)\n",
    "- **Ordering**: (asin, timestamp, user_id) for duplicate prevention\n",
    "- **Index Granularity**: 8192 (default, good for most use cases)\n",
    "\n",
    "### Duplicate Prevention Strategy\n",
    "\n",
    "1. **Primary Key**: `(asin, timestamp, user_id)` ensures no duplicate reviews\n",
    "2. **Data Checking**: Pre-ingestion verification of existing data\n",
    "3. **Batch Processing**: Efficient handling of large datasets\n",
    "4. **dbutils Integration**: Uses `sql_write()` with built-in duplicate handling\n",
    "\n",
    "### Performance Optimizations\n",
    "\n",
    "1. **Ordering Key**: `(asin, timestamp, user_id)` enables fast queries and prevents duplicates\n",
    "2. **Materialized Columns**: Pre-computed date fields avoid repeated calculations\n",
    "3. **Batch Processing**: 50,000 rows per batch for optimal memory usage\n",
    "4. **Data Cleaning**: NaN values handled to prevent insertion errors\n",
    "5. **Environment Configuration**: No hardcoded credentials, uses .env file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
