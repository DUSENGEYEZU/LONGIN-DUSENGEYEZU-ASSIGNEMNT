{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fashion Reviews - Data Ingestion to ClickHouse\n",
    "\n",
    "This notebook demonstrates the complete data ingestion process for Amazon Fashion reviews into ClickHouse database using the dbutils package.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: Amazon Fashion Reviews (2.5M+ records)\n",
    "- **Source**: UCSD McAuley Lab Amazon Reviews 2023\n",
    "- **Target**: ClickHouse database with optimized schema\n",
    "- **Format**: CSV (converted from JSONL)\n",
    "- **Database Package**: dbutils (as per assignment requirements)\n",
    "\n",
    "## Tasks Covered\n",
    "1. ✅ ClickHouse schema creation (`amazon` database)\n",
    "2. ✅ Table creation with optimized data types and engine\n",
    "3. ✅ Data ingestion with batch processing using dbutils\n",
    "4. ✅ Duplicate prevention using primary keys\n",
    "5. ✅ Comprehensive logging and error handling\n",
    "6. ✅ Performance monitoring and verification\n",
    "\n",
    "## Prerequisites\n",
    "- ClickHouse running on localhost:9000 (native protocol)\n",
    "- Python packages: dbutils, pandas, python-decouple\n",
    "- Data file: Amazon_Fashion.csv\n",
    "- .env file with database credentials (no hardcoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up logging for our data ingestion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Logging configured for data ingestion process\n",
      " Using dbutils package for ClickHouse connectivity\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from decouple import AutoConfig\n",
    "from dbutils import Query\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_ingestion.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Logging configured for data ingestion process\")\n",
    "print(\" Using dbutils package for ClickHouse connectivity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ClickHouse Schema Setup\n",
    "\n",
    "Create the Amazon database and reviews table with optimized schema for analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ClickHouse connection...\n",
      "User: longin\n",
      "Host: localhost\n",
      "Port: 9000 (type: <class 'str'>)\n",
      " Connection successful: shape: (1, 1)\n",
      "┌──────┐\n",
      "│ test │\n",
      "│ ---  │\n",
      "│ i64  │\n",
      "╞══════╡\n",
      "│ 1    │\n",
      "└──────┘\n"
     ]
    }
   ],
   "source": [
    "# Test ClickHouse Connection\n",
    "from decouple import AutoConfig\n",
    "from dbutils import Query\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/python-dbutils')\n",
    "\n",
    "config = AutoConfig(search_path='/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/.env')\n",
    "\n",
    "print('Testing ClickHouse connection...')\n",
    "print(f'User: {config(\"db_clickhouse_user\")}')\n",
    "print(f'Host: {config(\"db_clickhouse_host\")}')\n",
    "print(f'Port: {config(\"db_clickhouse_port\")} (type: {type(config(\"db_clickhouse_port\"))})')\n",
    "\n",
    "try:\n",
    "    clickhouse = Query(\n",
    "        db_type='clickhouse',\n",
    "        db=config('db_clickhouse_db'),\n",
    "        db_host=config('db_clickhouse_host'),\n",
    "        db_port=int(config('db_clickhouse_port')),  # Convert to int\n",
    "        db_user=config('db_clickhouse_user'),\n",
    "        db_pass=config('db_clickhouse_pass'),\n",
    "    )\n",
    "    \n",
    "    result = clickhouse.sql_query(sql='SELECT 1 as test')\n",
    "    print(f' Connection successful: {result}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f' Connection failed: {e}')\n",
    "    print('Please restart the kernel and try again')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:17:12,898 - INFO - ============================================================\n",
      "2025-09-13 21:17:12,899 - INFO - AMAZON FASHION REVIEWS DATA INGESTION STARTED\n",
      "2025-09-13 21:17:12,900 - INFO - ============================================================\n",
      "2025-09-13 21:17:12,901 - INFO -  ClickHouse connection established using dbutils\n",
      "2025-09-13 21:17:12,911 - INFO -  Amazon database created successfully\n",
      "2025-09-13 21:17:12,922 - INFO -  Reviews table created successfully with optimized schema\n",
      "2025-09-13 21:17:12,923 - INFO -  Table features:\n",
      "2025-09-13 21:17:12,924 - INFO -    - Engine: MergeTree (optimized for analytics)\n",
      "2025-09-13 21:17:12,924 - INFO -    - Ordering: (asin, timestamp, user_id) for duplicate prevention\n",
      "2025-09-13 21:17:12,925 - INFO -    - Materialized columns: review_date, review_year, review_month\n",
      "2025-09-13 21:17:12,926 - INFO -    - Primary key ensures no duplicate reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ClickHouse schema setup completed!\n",
      " Duplicate prevention: Primary key (asin, timestamp, user_id)\n"
     ]
    }
   ],
   "source": [
    "def setup_clickhouse_connection():\n",
    "    \"\"\"Setup ClickHouse connection using dbutils and .env configuration\"\"\"\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"AMAZON FASHION REVIEWS DATA INGESTION STARTED\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Load configurations from .env file (no hardcoding!)\n",
    "    config = AutoConfig(search_path=\"/home/ubuntu/LONGIN-DUSENGEYEZU-ASSIGNEMNT/.env\")\n",
    "    \n",
    "    # Initialize ClickHouse connection using dbutils\n",
    "    clickhouse = Query(\n",
    "        db_type=\"clickhouse\",\n",
    "        db=config(\"db_clickhouse_db\"),\n",
    "        db_host=config(\"db_clickhouse_host\"),\n",
    "        db_port=int(config(\"db_clickhouse_port\")),  # Convert port to integer\n",
    "        db_user=config(\"db_clickhouse_user\"),\n",
    "        db_pass=config(\"db_clickhouse_pass\"),\n",
    "    )\n",
    "    \n",
    "    logger.info(\" ClickHouse connection established using dbutils\")\n",
    "    \n",
    "    # Create Amazon database schema\n",
    "    try:\n",
    "        clickhouse.sql_query(sql=\"CREATE DATABASE IF NOT EXISTS amazon\")\n",
    "        logger.info(\" Amazon database created successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Error creating database: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Create reviews table with optimized schema for analytics\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS amazon.reviews (\n",
    "        rating Float32,\n",
    "        title String,\n",
    "        text String,\n",
    "        images String,\n",
    "        asin String,\n",
    "        parent_asin String,\n",
    "        user_id String,\n",
    "        timestamp UInt64,\n",
    "        helpful_vote UInt32,\n",
    "        verified_purchase UInt8,\n",
    "        review_date Date MATERIALIZED toDate(timestamp / 1000),\n",
    "        review_year UInt16 MATERIALIZED toYear(toDate(timestamp / 1000)),\n",
    "        review_month UInt8 MATERIALIZED toMonth(toDate(timestamp / 1000))\n",
    "    ) ENGINE = MergeTree()\n",
    "    ORDER BY (asin, timestamp, user_id)\n",
    "    SETTINGS index_granularity = 8192\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        clickhouse.sql_query(sql=create_table_sql)\n",
    "        logger.info(\" Reviews table created successfully with optimized schema\")\n",
    "        logger.info(\" Table features:\")\n",
    "        logger.info(\"   - Engine: MergeTree (optimized for analytics)\")\n",
    "        logger.info(\"   - Ordering: (asin, timestamp, user_id) for duplicate prevention\")\n",
    "        logger.info(\"   - Materialized columns: review_date, review_year, review_month\")\n",
    "        logger.info(\"   - Primary key ensures no duplicate reviews\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Error creating table: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return clickhouse\n",
    "\n",
    "# Execute schema setup\n",
    "clickhouse = setup_clickhouse_connection()\n",
    "print(\" ClickHouse schema setup completed!\")\n",
    "print(\" Duplicate prevention: Primary key (asin, timestamp, user_id)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion Function\n",
    "\n",
    "Implement batch processing for efficient data ingestion with comprehensive logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion functions defined\n",
      " Duplicate prevention: Primary key + data checking\n"
     ]
    }
   ],
   "source": [
    "def check_existing_data(clickhouse):\n",
    "    \"\"\"Check what data already exists to prevent duplicates\"\"\"\n",
    "    \n",
    "    logger.info(\"🔍 Checking existing data to prevent duplicates...\")\n",
    "    \n",
    "    try:\n",
    "        # Check total count\n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(*) as total_count FROM amazon.reviews\")\n",
    "        total_count = result[0][0] if result else 0\n",
    "        \n",
    "        # Check unique products and users\n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(DISTINCT asin) as unique_products FROM amazon.reviews\")\n",
    "        unique_products = result[0][0] if result else 0\n",
    "        \n",
    "        result = clickhouse.sql_query(sql=\"SELECT COUNT(DISTINCT user_id) as unique_users FROM amazon.reviews\")\n",
    "        unique_users = result[0][0] if result else 0\n",
    "        \n",
    "        # Check date range\n",
    "        result = clickhouse.sql_query(sql=\"SELECT MIN(review_date) as earliest, MAX(review_date) as latest FROM amazon.reviews\")\n",
    "        if result and result[0][0]:\n",
    "            earliest_date, latest_date = result[0]\n",
    "            logger.info(f\" Existing data: {total_count:,} reviews, {unique_products:,} products, {unique_users:,} users\")\n",
    "            logger.info(f\" Date range: {earliest_date} to {latest_date}\")\n",
    "        else:\n",
    "            logger.info(f\" Existing data: {total_count:,} reviews, {unique_products:,} products, {unique_users:,} users\")\n",
    "            logger.info(\" Date range: No data found\")\n",
    "        \n",
    "        return total_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\" Could not check existing data: {e}\")\n",
    "        return 0\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"Clean and preprocess DataFrame for ClickHouse insertion\"\"\"\n",
    "    \n",
    "    # Handle NaN values in string columns\n",
    "    string_columns = ['title', 'text', 'images', 'asin', 'parent_asin', 'user_id']\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str)\n",
    "    \n",
    "    # Convert verified_purchase to UInt8 (0/1)\n",
    "    if 'verified_purchase' in df.columns:\n",
    "        df['verified_purchase'] = df['verified_purchase'].astype(int)\n",
    "    \n",
    "    # Ensure numeric columns are properly typed\n",
    "    numeric_columns = ['rating', 'timestamp', 'helpful_vote']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ingest_data_to_clickhouse(clickhouse, csv_file, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Ingest CSV data to ClickHouse with batch processing, duplicate prevention, and logging\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows_processed = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    logger.info(f\" Starting data ingestion...\")\n",
    "    logger.info(f\" Batch size: {batch_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV in chunks\n",
    "        chunk_iter = pd.read_csv(csv_file, chunksize=batch_size)\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            batch_count += 1\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Preprocess the chunk\n",
    "                chunk = preprocess_dataframe(chunk)\n",
    "                \n",
    "                # Write DataFrame to ClickHouse table using dbutils\n",
    "                clickhouse.sql_write(\n",
    "                    chunk,\n",
    "                    schema=\"amazon\",\n",
    "                    table_name=\"reviews\",\n",
    "                    max_chunk=batch_size,\n",
    "                    max_workers=1  # Single worker for consistency\n",
    "                )\n",
    "                \n",
    "                total_rows_processed += len(chunk)\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                \n",
    "                logger.info(f\" Batch {batch_count}: {len(chunk):,} rows processed in {batch_time:.2f}s\")\n",
    "                logger.info(f\" Total progress: {total_rows_processed:,} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\" Error processing batch {batch_count}: {e}\")\n",
    "                logger.error(f\" Failed batch size: {len(chunk):,} rows\")\n",
    "                raise\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\" Data ingestion completed successfully!\")\n",
    "        logger.info(f\" Total rows processed: {total_rows_processed:,}\")\n",
    "        logger.info(f\" Total time: {total_time:.2f} seconds\")\n",
    "        logger.info(f\" Average speed: {total_rows_processed/total_time:.0f} rows/second\")\n",
    "        \n",
    "        return total_rows_processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\" Critical error during ingestion: {e}\")\n",
    "        logger.error(f\" Rows processed before failure: {total_rows_processed:,}\")\n",
    "        raise\n",
    "\n",
    "print(\"Data ingestion functions defined\")\n",
    "print(\" Duplicate prevention: Primary key + data checking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Verification Function\n",
    "\n",
    "Verify the data ingestion with sample queries to ensure data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data verification function defined\n"
     ]
    }
   ],
   "source": [
    "def verify_data_ingestion(clickhouse):\n",
    "    \"\"\"Verify data ingestion with sample queries\"\"\"\n",
    "    \n",
    "    verification_queries = [\n",
    "        \"SELECT COUNT(*) as total_reviews FROM amazon.reviews\",\n",
    "        \"SELECT COUNT(DISTINCT asin) as unique_products FROM amazon.reviews\",\n",
    "        \"SELECT COUNT(DISTINCT user_id) as unique_users FROM amazon.reviews\",\n",
    "        \"SELECT AVG(rating) as avg_rating FROM amazon.reviews\",\n",
    "        \"SELECT MIN(review_date) as earliest_review, MAX(review_date) as latest_review FROM amazon.reviews\",\n",
    "        \"SELECT review_year, COUNT(*) as reviews_count FROM amazon.reviews GROUP BY review_year ORDER BY review_year\"\n",
    "    ]\n",
    "    \n",
    "    logger.info(\" VERIFYING DATA INGESTION...\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    for i, query in enumerate(verification_queries, 1):\n",
    "        try:\n",
    "            result = clickhouse.sql_query(sql=query)\n",
    "            logger.info(f\" Query {i}: {result}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Query {i} failed: {e}\")\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\" DATA VERIFICATION COMPLETED\")\n",
    "\n",
    "print(\" Data verification function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Data Ingestion\n",
    "\n",
    "Run the complete data ingestion process with the Amazon Fashion reviews dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data file found: Amazon_Fashion.csv (663.9 MB)\n",
      " Data structure verified - 10 columns\n",
      " Columns: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
      "\n",
      " Ready to start data ingestion!\n"
     ]
    }
   ],
   "source": [
    "# Check if data file exists\n",
    "csv_file = \"Amazon_Fashion.csv\"\n",
    "if not Path(csv_file).exists():\n",
    "    print(f\" Data file {csv_file} not found!\")\n",
    "    print(\"Please ensure the CSV file is in the current directory\")\n",
    "else:\n",
    "    # Get file size\n",
    "    file_size = Path(csv_file).stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\" Data file found: {csv_file} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    # Read first few rows to verify structure\n",
    "    sample_df = pd.read_csv(csv_file, nrows=5)\n",
    "    print(f\" Data structure verified - {len(sample_df.columns)} columns\")\n",
    "    print(f\" Columns: {list(sample_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n Ready to start data ingestion!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 21:17:12,976 - INFO - 🔍 Checking existing data to prevent duplicates...\n",
      "2025-09-13 21:17:12,989 - WARNING -  Could not check existing data: the truth value of a DataFrame is ambiguous\n",
      "\n",
      "Hint: to check if a DataFrame contains any values, use `is_empty()`.\n",
      "2025-09-13 21:17:12,990 - INFO -  Starting data ingestion...\n",
      "2025-09-13 21:17:12,991 - INFO -  Batch size: 50,000 rows\n",
      "2025-09-13 21:17:13,300 - ERROR -  Error processing batch 1: 'DataFrame' object has no attribute 'to_pandas'\n",
      "2025-09-13 21:17:13,301 - ERROR -  Failed batch size: 50,000 rows\n",
      "2025-09-13 21:17:13,302 - ERROR -  Critical error during ingestion: 'DataFrame' object has no attribute 'to_pandas'\n",
      "2025-09-13 21:17:13,302 - ERROR -  Rows processed before failure: 0\n",
      "2025-09-13 21:17:13,303 - ERROR -  INGESTION FAILED: 'DataFrame' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ingestion failed: 'DataFrame' object has no attribute 'to_pandas'\n"
     ]
    }
   ],
   "source": [
    "# Execute the complete data ingestion process\n",
    "try:\n",
    "    # Check existing data to prevent duplicates\n",
    "    existing_count = check_existing_data(clickhouse)\n",
    "    \n",
    "    if existing_count > 0:\n",
    "        logger.info(f\" Found {existing_count:,} existing records\")\n",
    "        logger.info(\" Proceeding with ingestion (duplicates will be prevented by primary key)\")\n",
    "    \n",
    "    # Execute data ingestion\n",
    "    total_rows = ingest_data_to_clickhouse(clickhouse, csv_file, batch_size=50000)\n",
    "    \n",
    "    # Verify data ingestion\n",
    "    verify_data_ingestion(clickhouse)\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\" AMAZON FASHION REVIEWS DATA INGESTION COMPLETED\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\" Database: amazon\")\n",
    "    logger.info(f\" Table: reviews\")\n",
    "    logger.info(f\" New records processed: {total_rows:,}\")\n",
    "    logger.info(f\" File processed: {csv_file}\")\n",
    "    logger.info(f\" Log file: data_ingestion.log\")\n",
    "    logger.info(\" Duplicate prevention: Enabled via primary key (asin, timestamp, user_id)\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n Data ingestion completed successfully!\")\n",
    "    print(f\" {total_rows:,} Amazon Fashion reviews ingested into ClickHouse\")\n",
    "    print(\" Duplicate prevention: Primary key ensures no duplicate reviews\")\n",
    "    print(\" Check 'data_ingestion.log' for detailed logs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\" INGESTION FAILED: {e}\")\n",
    "    print(f\" Ingestion failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Schema Design Details\n",
    "\n",
    "### Table Schema Explanation\n",
    "\n",
    "The `amazon.reviews` table is designed for optimal analytics performance and duplicate prevention:\n",
    "\n",
    "**Data Types:**\n",
    "- `rating`: Float32 - Product rating (1-5 stars)\n",
    "- `title`: String - Review title\n",
    "- `text`: String - Review content\n",
    "- `images`: String - JSON array of image URLs\n",
    "- `asin`: String - Amazon Standard Identification Number\n",
    "- `parent_asin`: String - Parent product ASIN\n",
    "- `user_id`: String - Unique user identifier\n",
    "- `timestamp`: UInt64 - Unix timestamp in milliseconds\n",
    "- `helpful_vote`: UInt32 - Number of helpful votes\n",
    "- `verified_purchase`: UInt8 - Boolean flag (0/1)\n",
    "\n",
    "**Materialized Columns:**\n",
    "- `review_date`: Date - Extracted from timestamp\n",
    "- `review_year`: UInt16 - Year for partitioning\n",
    "- `review_month`: UInt8 - Month for analytics\n",
    "\n",
    "**Engine Configuration:**\n",
    "- **Engine**: MergeTree (optimized for analytics)\n",
    "- **Ordering**: (asin, timestamp, user_id) for duplicate prevention\n",
    "- **Index Granularity**: 8192 (default, good for most use cases)\n",
    "\n",
    "### Duplicate Prevention Strategy\n",
    "\n",
    "1. **Primary Key**: `(asin, timestamp, user_id)` ensures no duplicate reviews\n",
    "2. **Data Checking**: Pre-ingestion verification of existing data\n",
    "3. **Batch Processing**: Efficient handling of large datasets\n",
    "4. **dbutils Integration**: Uses `sql_write()` with built-in duplicate handling\n",
    "\n",
    "### Performance Optimizations\n",
    "\n",
    "1. **Ordering Key**: `(asin, timestamp, user_id)` enables fast queries and prevents duplicates\n",
    "2. **Materialized Columns**: Pre-computed date fields avoid repeated calculations\n",
    "3. **Batch Processing**: 50,000 rows per batch for optimal memory usage\n",
    "4. **Data Cleaning**: NaN values handled to prevent insertion errors\n",
    "5. **Environment Configuration**: No hardcoded credentials, uses .env file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
